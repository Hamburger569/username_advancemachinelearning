{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ukkz31BNBoEW"
      },
      "outputs": [],
      "source": [
        "#Import necessary libraries and modules from TensorFlow and Keras\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, LSTM\n",
        "from tensorflow.keras.preprocessing import sequence"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Download and extract the IMDB dataset.\n",
        "\n",
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JAxwh3lCAK5",
        "outputId": "8e8453b4-2791-47c0-8c9f-50bd17e13331"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  23.7M      0  0:00:03  0:00:03 --:--:-- 23.7M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the maximum number of features and the maximum length of sequences\n",
        "\n",
        "max_features = 10000\n",
        "maxlen = 150"
      ],
      "metadata": {
        "id": "XDmyUfEVCH-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)"
      ],
      "metadata": {
        "id": "rZSOH_5rCJmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "#Custom layer representing one layer of a transformer encoder\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "\n",
        "#Dense projection network\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "#Define how inputs are processed by the layer\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        " #Serialize the layer's configuration\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "on9oi5lRCSuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define parameters for the Transformer encoder layer\n",
        "\n",
        "vocab_size = 20000\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "\n",
        "#Defined a model architecture using an embedding layer followed by a Transformer encoder layer and other dense layers.\n",
        "#Compiling the model with optimizer, loss function, and metrics\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwjU7K2OCVQH",
        "outputId": "00138541-ce46-4af5-a688-b2db5caf7b7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 256)         5120000   \n",
            "                                                                 \n",
            " transformer_encoder (Trans  (None, None, 256)         543776    \n",
            " formerEncoder)                                                  \n",
            "                                                                 \n",
            " global_max_pooling1d (Glob  (None, 256)               0         \n",
            " alMaxPooling1D)                                                 \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5664033 (21.61 MB)\n",
            "Trainable params: 5664033 (21.61 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "#Defining a class Vectorizer with a method \"standardize\" to preprocess text by converting it to lowercase and removing punctuation.\n",
        "class Vectorizer:\n",
        "    def standardize(self, text):\n",
        "        text = text.lower()\n",
        "        return \"\".join(char for char in text if char not in string.punctuation)"
      ],
      "metadata": {
        "id": "xCeh_6MiCb7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define a method \"tokenize\"\n",
        "\n",
        "def tokenize(self, text):\n",
        "        text = self.standardize(text)\n",
        "        return text.split()"
      ],
      "metadata": {
        "id": "lszce6usChlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a vocabulary from the dataset through a method \"make_vocabulary\"\n",
        "\n",
        "def make_vocabulary(self, dataset):\n",
        "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
        "        for text in dataset:\n",
        "            text = self.standardize(text)\n",
        "            tokens = self.tokenize(text)\n",
        "            for token in tokens:\n",
        "                if token not in self.vocabulary:\n",
        "                    self.vocabulary[token] = len(self.vocabulary)\n",
        "        self.inverse_vocabulary = dict(\n",
        "            (v, k) for k, v in self.vocabulary.items())"
      ],
      "metadata": {
        "id": "BLbrpPGrCk59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining a method \"encode\" to encode text using the created vocabulary\n",
        "\n",
        "def encode(self, text):\n",
        "        text = self.standardize(text)\n",
        "        tokens = self.tokenize(text)\n",
        "        return [self.vocabulary.get(token, 1) for token in tokens]"
      ],
      "metadata": {
        "id": "qePzlVMgCox9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Decoding integer sequences back to text\n",
        "\n",
        "def decode(self, int_sequence):\n",
        "        return \" \".join(\n",
        "            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)"
      ],
      "metadata": {
        "id": "FbPyCPSPCs8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "JB_nfbJmC1Uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using a custom standardization function for text preprocessing using TensorFlow\n",
        "\n",
        "def custom_standardization_fn(string_tensor):\n",
        "    lowercase_string = tf.strings.lower(string_tensor)\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\")"
      ],
      "metadata": {
        "id": "tNaTKhM7C33V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using a custom split function for text preprocessing\n",
        "\n",
        "def custom_split_fn(string_tensor):\n",
        "    return tf.strings.split(string_tensor)\n"
      ],
      "metadata": {
        "id": "Hk4xt1q8C7JI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing the unsupervised training data directory, displaying the content of a positive review file\n",
        "\n",
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz\n",
        "!rm -r aclImdb/train/unsup\n",
        "!cat aclImdb/train/pos/4077_10.txt\n",
        "import os, pathlib, shutil, random"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPjt7vFPC_jw",
        "outputId": "04d1f55a-939c-41bc-c6c1-ec2d7a6aeb6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  22.9M      0  0:00:03  0:00:03 --:--:-- 22.9M\n",
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining batch size and directories for the dataset\n",
        "\n",
        "batch_size = 32\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\""
      ],
      "metadata": {
        "id": "kUxjeLuIJXYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Moving a portion of training data to the validation directory for each category\n",
        "#Here, I am making sure the directory and file exist\n",
        "\n",
        "for category in (\"neg\", \"pos\"):\n",
        "\n",
        "    if not os.path.exists(val_dir / category):\n",
        "        os.makedirs(val_dir / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "\n",
        "        if not os.path.exists(val_dir / category / fname):\n",
        "            shutil.move(train_dir / category / fname,\n",
        "                        val_dir / category / fname)"
      ],
      "metadata": {
        "id": "ALP5qFd4IuS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "ZfyrRXZrDIsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating TensorFlow datasets for training, validation, and testing\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAD_T089DOHs",
        "outputId": "85187f4c-d69f-4247-b629-6e5e3681c977"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Displaying the shapes and dtypes of the first batch\n",
        "\n",
        "for inputs, targets in train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIhrVM_tDQfS",
        "outputId": "dd1dd165-292a-413a-81de-75c33aa55846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32,)\n",
            "inputs.dtype: <dtype: 'string'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor(b'It was once suggested by Pauline Kael, never a fan, that Cassavetes thought not like a director, but like an actor. What Kael meant was his supposed lack of sophistication as a filmmaker; to take that comparison further, to me, it never feels like Cassavetes is directing himself in a film, it feels like Cassavetes implanting himself inside his own creation, like Orson Welles. Cassavetes is just as much of a genius as Welles, but far more important as a true artist (as opposed to a technician or rhetorician). This is like a cross between Italian passion (though Cassavetes was actually Greek) and Scandinavian introversion. Never before have inner demons been so exposed physically.<br /><br />It\\'s about the mystery of becoming, performing, and acting. Like a haunted Skip James record, it\\'s got the echoes of ghosts all around. Rowlands\\' breakdowns, which are stupefying and almost operatic, surprising coming from Cassavetes, are accompanied by a jumpy, unsettling piano. Who is this dead girl? The metaphysical possibilities are endless, and it\\'s amazing to find this kind of thing in a Cassavetes film, just the overt display of intelligence (there is also a brief bit of voice-over at the beginning). But then, he always was intelligent, he just never flapped it around for easy praise. This is not \"Adaptation\"; here, the blending of reality and fiction and drama is not to show cleverness but to show the inner turmoil and confusion it creates.<br /><br />There\\'s so much going on. The pure, joyous love when Rowlands greets her doorman; the horror when she beats herself up... The scene where the girl talks about how she devoted her life to art and to music is one of the most effective demonstrations of understanding what it means to be a fan of someone. You can see some roots of this in \"A Star Is Born,\" and Almodovar borrowed from it for \"All About My Mother.\" I think the ending is a little bit of a disappointment because of the laughing fits, but the preparation leading up to it is almost sickening. (You can shoot me, but I think the alcoholism, despite its urgency in many of the scenes, is a relatively small point about the film.)<br /><br />It\\'s a living, breathing thing, and it feels like a process: it could go any direction at any time. Like \"Taste of Cherry,\" we are reminded that \"you must never forget this is only a play.\" Yet it is dangerous: when Rowlands says that line, is it great drama? How will the audience take it? Is she being reflexive or does she just not care? Her (character\\'s) breakdowns are incorporated into the performances, and ultimately the film, in such a way that it\\'s like witnessing a female James Dean. 10/10', shape=(), dtype=string)\n",
            "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import TextVectorization"
      ],
      "metadata": {
        "id": "Pw0MLlRkD2Cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a dataset containing only text inputs for training\n",
        "\n",
        "text_vectorization = TextVectorization(\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"multi_hot\",\n",
        ")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "text_vectorization.adapt(text_only_train_ds)"
      ],
      "metadata": {
        "id": "PB7oSYgaD5c7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using the text_vectorization function to convert text data into binary 1-gram representations\n",
        "\n",
        "#Here, the use of \"num_parallel_calls=4\" argument indicates mapping done in parallel using four threads\n",
        "\n",
        "binary_1gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_1gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_1gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)"
      ],
      "metadata": {
        "id": "ZlSceBMrEIer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Exploration in binary_1gram_train_ds\n",
        "\n",
        "for inputs, targets in binary_1gram_train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwykvQhnEKPH",
        "outputId": "18e53f69-8621-48b3-d522-5179cb89d13e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32, 20000)\n",
            "inputs.dtype: <dtype: 'float32'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
            "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "1X9GaOtuEOVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to Build and Compile a Feedforward Neural Network Model\n",
        "#max_tokens (int) indicates maximum number of tokens in each input, while sequence.hidden_dim (int) indicates dimensionality of the hidden layer.\n",
        "\n",
        "def get_model(max_tokens=20000, hidden_dim=16):\n",
        "    inputs = keras.Input(shape=(max_tokens,))\n",
        "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=\"rmsprop\",\n",
        "                  loss=\"binary_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model"
      ],
      "metadata": {
        "id": "PhKtP8j5ESnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating and summarizing a simple dense model.\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYr1uILjEYN0",
        "outputId": "f9d3e6f4-d164-49ba-eb3f-d7733486d5e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320033 (1.22 MB)\n",
            "Trainable params: 320033 (1.22 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Training the model using the integer encoded training and validation dataset\n",
        "\n",
        "model.fit(binary_1gram_train_ds.cache(),\n",
        "          validation_data=binary_1gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "\n",
        "#Loading the model and evaluating its performance on the test dataset\n",
        "model = keras.models.load_model(\"binary_1gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ybo0hbKEmD8",
        "outputId": "d067544e-8a22-40b5-d060-2df0dff81354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "782/782 [==============================] - 25s 31ms/step - loss: 0.3940 - accuracy: 0.8357 - val_loss: 0.2205 - val_accuracy: 0.9194\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.2755 - accuracy: 0.8986 - val_loss: 0.1841 - val_accuracy: 0.9356\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.2517 - accuracy: 0.9127 - val_loss: 0.1710 - val_accuracy: 0.9456\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.2378 - accuracy: 0.9202 - val_loss: 0.1557 - val_accuracy: 0.9494\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.2336 - accuracy: 0.9229 - val_loss: 0.1484 - val_accuracy: 0.9538\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.2297 - accuracy: 0.9250 - val_loss: 0.1537 - val_accuracy: 0.9544\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.2273 - accuracy: 0.9271 - val_loss: 0.1432 - val_accuracy: 0.9544\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.2255 - accuracy: 0.9269 - val_loss: 0.1499 - val_accuracy: 0.9562\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.2183 - accuracy: 0.9301 - val_loss: 0.1433 - val_accuracy: 0.9532\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.2178 - accuracy: 0.9292 - val_loss: 0.1420 - val_accuracy: 0.9580\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.3619 - accuracy: 0.8822\n",
            "Test acc: 0.882\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Configuring TextVectorization Parameters\n",
        "\n",
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"multi_hot\",\n",
        ")"
      ],
      "metadata": {
        "id": "-WUAGu24E7Ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training and testing the binary bigram model\n",
        "\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "binary_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)"
      ],
      "metadata": {
        "id": "L5iMa_2KE_F6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initializing Model and Setting Up Checkpoints\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3G90MiMFJ_z",
        "outputId": "6af0d3e2-9a2a-4021-919e-ae7dacc684d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320033 (1.22 MB)\n",
            "Trainable params: 320033 (1.22 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Training the Model and Evaluating Test Accuracy\n",
        "\n",
        "model.fit(binary_2gram_train_ds.cache(),\n",
        "          validation_data=binary_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUpYQrNvFLxZ",
        "outputId": "1f0c05a6-0fde-4bd3-884f-40e28df6c9ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "782/782 [==============================] - 14s 18ms/step - loss: 0.3688 - accuracy: 0.8512 - val_loss: 0.1890 - val_accuracy: 0.9362\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.2433 - accuracy: 0.9156 - val_loss: 0.1540 - val_accuracy: 0.9444\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.2183 - accuracy: 0.9294 - val_loss: 0.1271 - val_accuracy: 0.9596\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.2059 - accuracy: 0.9364 - val_loss: 0.1115 - val_accuracy: 0.9640\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.1991 - accuracy: 0.9412 - val_loss: 0.1094 - val_accuracy: 0.9622\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.1973 - accuracy: 0.9446 - val_loss: 0.1070 - val_accuracy: 0.9646\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.1901 - accuracy: 0.9462 - val_loss: 0.0985 - val_accuracy: 0.9688\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 0.1848 - accuracy: 0.9470 - val_loss: 0.0940 - val_accuracy: 0.9692\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.1879 - accuracy: 0.9466 - val_loss: 0.1004 - val_accuracy: 0.9662\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.1779 - accuracy: 0.9492 - val_loss: 0.0924 - val_accuracy: 0.9672\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.3633 - accuracy: 0.8841\n",
            "Test acc: 0.884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Configuring the TextVectorization layer to return token counts\n",
        "\n",
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"count\"\n",
        ")"
      ],
      "metadata": {
        "id": "tTreNxg4Fd5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Configuring TextVectorization to return TF-IDF-weighted outputs\n",
        "\n",
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"tf_idf\",\n",
        ")"
      ],
      "metadata": {
        "id": "ObHDunQpFhcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training and testing the TF-IDF bigram model\n",
        "\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "tfidf_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "tfidf_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "tfidf_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n"
      ],
      "metadata": {
        "id": "MGSoRCayFlSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initializing the Model and Summary\n",
        "model = get_model()\n",
        "model.summary()\n",
        "\n",
        "#Model Checkpoint Configuration\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaD700IdFuwI",
        "outputId": "a0dd2491-a0b2-43d2-dd81-b79d6030a82e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320033 (1.22 MB)\n",
            "Trainable params: 320033 (1.22 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Training and Evaluation on Test Data\n",
        "\n",
        "model.fit(tfidf_2gram_train_ds.cache(),\n",
        "          validation_data=tfidf_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVTwh8wXFwwj",
        "outputId": "ea19519c-42d7-46c9-d9fe-cc31a46fc0d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "782/782 [==============================] - 13s 16ms/step - loss: 0.5070 - accuracy: 0.7670 - val_loss: 0.3083 - val_accuracy: 0.8842\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.3605 - accuracy: 0.8458 - val_loss: 0.1952 - val_accuracy: 0.9232\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.3236 - accuracy: 0.8568 - val_loss: 0.1831 - val_accuracy: 0.9390\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.2999 - accuracy: 0.8645 - val_loss: 0.1725 - val_accuracy: 0.9372\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.2852 - accuracy: 0.8691 - val_loss: 0.1740 - val_accuracy: 0.9276\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 0.2730 - accuracy: 0.8788 - val_loss: 0.1467 - val_accuracy: 0.9476\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.2637 - accuracy: 0.8864 - val_loss: 0.1443 - val_accuracy: 0.9468\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.2525 - accuracy: 0.8974 - val_loss: 0.1525 - val_accuracy: 0.9410\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 5s 6ms/step - loss: 0.2416 - accuracy: 0.9001 - val_loss: 0.1316 - val_accuracy: 0.9504\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 6s 7ms/step - loss: 0.2341 - accuracy: 0.9014 - val_loss: 0.1300 - val_accuracy: 0.9498\n",
            "782/782 [==============================] - 5s 7ms/step - loss: 0.3626 - accuracy: 0.8697\n",
            "Test acc: 0.870\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Inference and Prediction\n",
        "\n",
        "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "processed_inputs = text_vectorization(inputs)\n",
        "outputs = model(processed_inputs)\n",
        "inference_model = keras.Model(inputs, outputs)\n",
        "import tensorflow as tf\n",
        "raw_text_data = tf.convert_to_tensor([\n",
        "    [\"That was an excellent movie, I loved it.\"],\n",
        "])\n",
        "predictions = inference_model(raw_text_data)\n",
        "print(f\"{float(predictions[0] * 100):.2f} percent positive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TArf0CmuGIfq",
        "outputId": "9f600f49-829e-4148-a04d-447163d3556c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99.74 percent positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Processing words as a sequence: The sequence model approach\n",
        "\n",
        "#Downloading the data\n",
        "\n",
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz\n",
        "!rm -r aclImdb/train/unsup"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAzNe6VCGNca",
        "outputId": "6748f12e-dbe6-464b-ded3-23f2266b41ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  15.7M      0  0:00:05  0:00:05 --:--:-- 16.1M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Preparing the data\n",
        "\n",
        "import os, pathlib, shutil, random\n",
        "from tensorflow import keras\n"
      ],
      "metadata": {
        "id": "GkHwvfUBGbJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting up directories\n",
        "\n",
        "batch_size = 32\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\""
      ],
      "metadata": {
        "id": "PYS82XM5Gv6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for category in (\"neg\", \"pos\"):\n",
        "    # Check if the directory already exists\n",
        "    if not os.path.exists(val_dir / category):\n",
        "        os.makedirs(val_dir / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        # Check if the file already exists in the destination directory\n",
        "        if not os.path.exists(val_dir / category / fname):\n",
        "            shutil.move(train_dir / category / fname,\n",
        "                        val_dir / category / fname)"
      ],
      "metadata": {
        "id": "anQ9FfSEHGH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating Datasets from Directories\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xwj7uaQLHKRG",
        "outputId": "5a1407fa-bde7-4608-f2b5-205747f616a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Preparing integer sequence datasets\n",
        "\n",
        "from tensorflow.keras import layers\n"
      ],
      "metadata": {
        "id": "TSh6xvEBHacB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Text Vectorization: Initialization and Adaptation\n",
        "\n",
        "max_length = 600\n",
        "max_tokens = 20000\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_length,\n",
        ")\n",
        "text_vectorization.adapt(text_only_train_ds)"
      ],
      "metadata": {
        "id": "4kcCQDI8Hfof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Integer Encoding Mapping for Datasets\n",
        "\n",
        "int_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)"
      ],
      "metadata": {
        "id": "andYP3k4HngJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#A sequence model built on one-hot encoded vector sequences\n",
        "\n",
        "import tensorflow as tf\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = tf.one_hot(inputs, depth=max_tokens)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "WaeJwhyCHpB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92AwmMF4HyT7",
        "outputId": "97b2e2d3-fd29-4c80-c130-4191b818dc65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_8 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " tf.one_hot (TFOpLambda)     (None, None, 20000)       0         \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 64)                5128448   \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5128513 (19.56 MB)\n",
            "Trainable params: 5128513 (19.56 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "xSNNQZowOdNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "f8qYcKnjOhkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a simple neural network model for binary classification tasks to specify the maximum number of input tokens and the size of the hidden layer\n",
        "\n",
        "def get_model(max_tokens=20000, hidden_dim=16):\n",
        "    inputs = keras.Input(shape=(max_tokens,))\n",
        "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=\"rmsprop\",\n",
        "                  loss=\"binary_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model"
      ],
      "metadata": {
        "id": "cnvA53CqOubg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = get_model()\n"
      ],
      "metadata": {
        "id": "OqPJ4bpMOzdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Callback for Model Checkpointing\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\",\n",
        "                                    save_best_only=True)\n",
        "]"
      ],
      "metadata": {
        "id": "G_hoA5sIO31a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import pathlib\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "\n"
      ],
      "metadata": {
        "id": "IZx2QmgnO729"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the base directory\n",
        "base_dir = pathlib.Path(\"aclImdb\")"
      ],
      "metadata": {
        "id": "BHhfbWK9Ri03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the directories for train, validation, and test sets\n",
        "train_dir = base_dir / \"train\"\n",
        "val_dir = base_dir / \"val\"\n",
        "test_dir = base_dir / \"test\""
      ],
      "metadata": {
        "id": "J_dlOseURmW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create directories for validation set\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category, exist_ok=True)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname, val_dir / category / fname)\n"
      ],
      "metadata": {
        "id": "j_YooQp7RqrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Preparation for Text Classification\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    train_dir, batch_size=batch_size, validation_split=0.2, subset=\"training\", seed=1337\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    train_dir, batch_size=batch_size, validation_split=0.2, subset=\"validation\", seed=1337\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    test_dir, batch_size=batch_size\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8KOfybjRu-T",
        "outputId": "ce231220-3d5a-464f-c72e-61f2dd6632b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Using 16000 files for training.\n",
            "Found 20000 files belonging to 2 classes.\n",
            "Using 4000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining the TextVectorization layer\n",
        "max_features = 20000\n",
        "sequence_length = 600\n",
        "\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n"
      ],
      "metadata": {
        "id": "TCpWG6CwR06q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Adapting the TextVectorization layer on the training dataset\n",
        "train_text = train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(train_text)\n"
      ],
      "metadata": {
        "id": "IOrRz545R5Z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the model\n",
        "embedding_dim = 128\n",
        "model = keras.Sequential([\n",
        "    vectorize_layer,\n",
        "    layers.Embedding(max_features + 1, embedding_dim),\n",
        "    layers.Bidirectional(layers.LSTM(64)),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(1)\n",
        "])"
      ],
      "metadata": {
        "id": "kXjCv1XRSDXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compile the model\n",
        "model.compile(\n",
        "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.Adam(1e-4),\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "VwLIkuCMSH4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model\n",
        "model.fit(train_ds, validation_data=val_ds, epochs=10)\n",
        "\n",
        "#Evaluate the model\n",
        "print(\"Evaluate on test data\")\n",
        "results = model.evaluate(test_ds)\n",
        "print(\"Test loss, Test accuracy:\", results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzBY7aB9SMy2",
        "outputId": "724efd3a-41bb-4d91-b0cf-5a4edef22a92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "500/500 [==============================] - 413s 816ms/step - loss: 0.6190 - accuracy: 0.5858 - val_loss: 0.4190 - val_accuracy: 0.8320\n",
            "Epoch 2/10\n",
            "500/500 [==============================] - 383s 767ms/step - loss: 0.2934 - accuracy: 0.8811 - val_loss: 0.3118 - val_accuracy: 0.8783\n",
            "Epoch 3/10\n",
            "500/500 [==============================] - 406s 813ms/step - loss: 0.1912 - accuracy: 0.9300 - val_loss: 0.3127 - val_accuracy: 0.8708\n",
            "Epoch 4/10\n",
            "500/500 [==============================] - 386s 772ms/step - loss: 0.1307 - accuracy: 0.9554 - val_loss: 0.3325 - val_accuracy: 0.8788\n",
            "Epoch 5/10\n",
            "500/500 [==============================] - 382s 764ms/step - loss: 0.0873 - accuracy: 0.9721 - val_loss: 0.3753 - val_accuracy: 0.8832\n",
            "Epoch 6/10\n",
            "500/500 [==============================] - 406s 813ms/step - loss: 0.0581 - accuracy: 0.9836 - val_loss: 0.4378 - val_accuracy: 0.8742\n",
            "Epoch 7/10\n",
            "500/500 [==============================] - 385s 770ms/step - loss: 0.0379 - accuracy: 0.9891 - val_loss: 0.4297 - val_accuracy: 0.8730\n",
            "Epoch 8/10\n",
            "500/500 [==============================] - 386s 772ms/step - loss: 0.0313 - accuracy: 0.9907 - val_loss: 0.6503 - val_accuracy: 0.8675\n",
            "Epoch 9/10\n",
            "500/500 [==============================] - 387s 773ms/step - loss: 0.0212 - accuracy: 0.9948 - val_loss: 0.5805 - val_accuracy: 0.8640\n",
            "Epoch 10/10\n",
            "500/500 [==============================] - 409s 817ms/step - loss: 0.0099 - accuracy: 0.9980 - val_loss: 0.6484 - val_accuracy: 0.8660\n",
            "Evaluate on test data\n",
            "782/782 [==============================] - 136s 174ms/step - loss: 0.7339 - accuracy: 0.8495\n",
            "Test loss, Test accuracy: [0.7338881492614746, 0.8494799733161926]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_tokens = 20000  # Defining the maximum number of tokens in your vocabulary\n",
        "embedding_layer = layers.Embedding(input_dim=max_tokens, output_dim=256)\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "1cjCNl77j3c5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model definition with embedding layer for text classification\n",
        "\n",
        "embedding_layer = layers.Embedding(input_dim=max_tokens, output_dim=256)\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n"
      ],
      "metadata": {
        "id": "X7IE3guTkD3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "086k2WE1kHyA",
        "outputId": "7f253d86-fcb4-41cd-fcd1-0d0bdf974442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_4 (Embedding)     (None, None, 256)         5120000   \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirecti  (None, 64)                73984     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5194049 (19.81 MB)\n",
            "Trainable params: 5194049 (19.81 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Directory paths for training, validation, and test data\n",
        "\n",
        "train_dir = base_dir / \"train\"\n",
        "val_dir = base_dir / \"val\"\n",
        "test_dir = base_dir / \"test\"\n",
        "\n"
      ],
      "metadata": {
        "id": "rXT3ryRJk12m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model architecture definition with embedding layer, bidirectional LSTM, dropout, and dense layer\n",
        "\n",
        "embedding_layer = layers.Embedding(input_dim=max_tokens, output_dim=256)"
      ],
      "metadata": {
        "id": "wQ0PedtCls4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model input and layers configuration: embedding, bidirectional LSTM, dropout, and dense layers\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "YyYnr5Qzl4nS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model compilation with RMSprop optimizer, binary crossentropy loss, and accuracy metric\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n"
      ],
      "metadata": {
        "id": "6bW22Yf5l8zy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}